{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"/home/vijay/train.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Technical writing is a product-focused job. A technical writer's main role is to deliver complex information, instructions, ideas and functions into comprehensible words for the audience. Usually, that involves a deep understanding of a certain product and being able to explain it easily for users to use it. On the other hand, technical blogging is more audience-focused. A technical blogger is a content writer that focuses on delivering high-quality articles that engages their audience and brings traffic to their business' website. They often write about topics that their target audience are interested about and provide value from their background technical knowledge or with a little research. Because technical writing is product-focused, it mostly involves a specific manual or instruction on how to use the product. The content they write must be clear, concise, objective and rigid. The style and tone is neutral, formal and professional. Any images you see in technical writings will be diagrams, tables, charts that provide useful information about the product. Technical bloggers write content to engage their audiences. Hence, their content does not have to be as objective and rigid, but the topic needs to be current, relevant and SEO optimized. Unlike technical writings, most technical blogs include plenty of keywords, visually appealing pictures, infographics and have a writing style that is more casual, as if the writer is talking to you. Technical writers often only focus on a niche topic, one that is related to the product they are writing about. If the company they are writing for develops a software, they will most likely write documentations and technical reports on that software. They will need to write at a deeper level of understanding than a technical blogger. As a result, they become specialists in their own niche, having depth over breadth of technical knowledge. Technical bloggers usually cover a range of topics. However, that does not mean they don't have a deep understanding on the product. It is just not as specialized as the level of technical writing. They can write high-quality, in-depth content for companies' blogs, reviews, social media posts and pretty much any marketing content that can garner a lot of audience interest. Ultimately, their priority is to leverage the broad range of topics they write to reach a wider audience within their target industry. You don’t have to work in the technology industry or offer image moderation software or video moderation API like we do to benefit from the best tech blogs today. Technological innovations are always surfacing, and some of them will have a tremendous impact on your business. If you’re one of the first to embrace high-tech discoveries, you’re more likely to beat your competitors and dominate your market. Tech blogs can also deliver fresh topic ideas for blog posts, website content, and social media posts. Find the intersection between a recent post on one of the top tech blogs and something that is of interest to your audience. That’s all you need for a creative idea that will bring attention and authority to your brand. In hopes of lighting your content marketing strategy on fire, we created this technology blogs list to help you find the most influential tech bloggers to follow for your business. You may also find some of these information technology blogs interesting from a personal perspective, so dig in and learn something new today. Are you aware of the most groundbreaking technological innovations on the horizon? If not, get connected with Venture Beat right away. We feel it is one of the best tech blogs for people who want to hear about the latest scientific studies as well as the daily headlines circulating the technology world. The online technology magazine offers a variety of new topics daily, covering the technology that may change your business and personal life tomorrow. Venture Beat covers everything from video games and artificial intelligence to marketing, media, and transportation. Following the channels that are relevant to your professional and personal life allows you to save time while missing nothing. Immerse yourself in the business end of the technology field with one of the most popular tech blogs thriving in 2020. The blog reads like a magazine with topics covering lawsuits, stock market IPOs, and other business dealings of leading companies around the world. You’ll also learn a little about credit cards, apps, and various new gadgets while seeing what some of your biggest competitors are doing to dominate your market. Sign up for the TechCrunch newsletter to have updates sent right to your mailbox. Consider signing up for the Extra Crunch membership if you would benefit from detailed overviews of new startups, business-building resources, and access to industry-approved lists of verified service providers. The GeekWire blog is where many modern geeks go to get wired with technology information in 2020. They present the most prominent tech industry news daily as well as some original content that may help you keep up to date with technology innovations impacting your business and personal life. You may also want to explore the resources tab for valuable information dedicated to companies and professionals in the tech field. Consider attending a GeekWire event for opportunities to discuss cutting-edge digital trends with industry experts. The events are also a great place to shop for content topic ideas that will appeal to your professional audience. You can give updated reports in true journalistic style if your audience appreciates any of the topics presented at upcoming events. TechSpot focuses heavily on consumer electronics. The blog allows you to browse product reviews, news updates, and trending stories with less frustration. If you manage or create online content around specific types of tech products, check out the TechSpot The Best pages. You’ll find detailed overviews of the best tech products in a variety of categories with the information presented from industry professionals. Do you like to chat with other people interested in the technology field? Visit the TechSpot forums to ask questions or pick up quick tips from a highly informed community. The forum is a great place to discuss stories that you see in the TechSpot blog. Problems surfacing in the forum may give you ideas for items that you need to answer for your audience in a way that is relevant to your business. BGR maintains a focus on mobile technology and consumer electronics. The site features a variety of timely topics that will help you understand the latest technology innovations while making smart tech choices in daily life. It’s one of the best tech blogs 2020 has to offer because the news updates, product reviews, and shopping deals are always relevant to a broad audience. If you don’t know much about technology right now, this is one of the best tech blogs to follow daily. You can keep up with tech news in less time by looking at the trending list featured on the front page of the BGR website at least once a day. It will give you a quick rundown of the current digital trends in exchange for a few moments of your time. If your business relates to IT in any way, this is one of the leading information technology blogs for 2020. The founders view technology as an art form, and they use their online space to present topics of interest to IT professionals everywhere. The blog presents a diverse range of ideas on various political and cultural topics too. Reading new Ars Technica posts often is a great way to keep up with the world that isn’t in your backyard, city, or state. You may not always like what they have to say at Ars Technica, but they will certainly give you a lot to think about and perhaps a few things to discuss with your online audience. SiliconANGLE is one of the most popular tech blogs for enterprise-level professionals who make high-tech decisions. It’s also a great place to catch up on trending topics in the technology field, so it appeals to anyone who wants to know more about innovations coming up in the tech field. The blog features trending stories on the front page of the website, but you can also explore sections of content dedicated to the cloud, Internet of Things, artificial intelligence, blockchain, security, and big data. A special section devoted to emerging tech is a great place to find more information on technology that may impact your field of business.SiliconANGLE is one of the most popular tech blogs for enterprise-level professionals who make high-tech decisions. It’s also a great place to catch up on trending topics in the technology field, so it appeals to anyone who wants to know more about innovations coming up in the tech field. The blog features trending stories on the front page of the website, but you can also explore sections of content dedicated to the cloud, Internet of Things, artificial intelligence, blockchain, security, and big data. A special section devoted to emerging tech is a great place to find more information on technology that may impact your field of business.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9169"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 46, 8, 3, 29, 79, 223, 3, 15, 224, 225, 226, 8, 2, 122]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "# print(tokenizer)\n",
    "\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1525"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  1522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 15,  46,   8,   3],\n",
       "       [ 46,   8,   3,  29],\n",
       "       [  8,   3,  29,  79],\n",
       "       [  3,  29,  79, 223],\n",
       "       [ 29,  79, 223,   3],\n",
       "       [ 79, 223,   3,  15],\n",
       "       [223,   3,  15, 224],\n",
       "       [  3,  15, 224, 225],\n",
       "       [ 15, 224, 225, 226],\n",
       "       [224, 225, 226,   8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[ 15  46   8]\n",
      " [ 46   8   3]\n",
      " [  8   3  29]\n",
      " [  3  29  79]\n",
      " [ 29  79 223]\n",
      " [ 79 223   3]\n",
      " [223   3  15]\n",
      " [  3  15 224]\n",
      " [ 15 224 225]\n",
      " [224 225 226]]\n",
      "Response:  [  3  29  79 223   3  15 224 225 226   8]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "# y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1, 10)             4910      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 491)               491491    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,545,401\n",
      "Trainable params: 13,545,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 3).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_1_input'), name='embedding_1_input', description=\"created by layer 'embedding_1_input'\"), but it was called on an input with incompatible shape (None, 3).\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.9242\n",
      "Epoch 00001: loss improved from inf to 5.92417, saving model to next_words.h5\n",
      "24/24 [==============================] - 9s 286ms/step - loss: 5.9242\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.5430\n",
      "Epoch 00002: loss improved from 5.92417 to 5.54302, saving model to next_words.h5\n",
      "24/24 [==============================] - 6s 262ms/step - loss: 5.5430\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.4580\n",
      "Epoch 00003: loss improved from 5.54302 to 5.45795, saving model to next_words.h5\n",
      "24/24 [==============================] - 6s 262ms/step - loss: 5.4580\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.4008\n",
      "Epoch 00004: loss improved from 5.45795 to 5.40076, saving model to next_words.h5\n",
      "24/24 [==============================] - 8s 318ms/step - loss: 5.4008\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.4119\n",
      "Epoch 00005: loss did not improve from 5.40076\n",
      "24/24 [==============================] - 5s 202ms/step - loss: 5.4119\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.2956\n",
      "Epoch 00006: loss improved from 5.40076 to 5.29560, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 297ms/step - loss: 5.2956\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.2248\n",
      "Epoch 00007: loss improved from 5.29560 to 5.22479, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 282ms/step - loss: 5.2248\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.1493\n",
      "Epoch 00008: loss improved from 5.22479 to 5.14931, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 282ms/step - loss: 5.1493\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.0914\n",
      "Epoch 00009: loss improved from 5.14931 to 5.09144, saving model to next_words.h5\n",
      "24/24 [==============================] - 6s 270ms/step - loss: 5.0914\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 5.0432- ETA: 0s - loss: 5.0\n",
      "Epoch 00010: loss improved from 5.09144 to 5.04316, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 295ms/step - loss: 5.0432\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.9884\n",
      "Epoch 00011: loss improved from 5.04316 to 4.98845, saving model to next_words.h5\n",
      "24/24 [==============================] - 6s 267ms/step - loss: 4.9884\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.9481\n",
      "Epoch 00012: loss improved from 4.98845 to 4.94811, saving model to next_words.h5\n",
      "24/24 [==============================] - 6s 259ms/step - loss: 4.9481\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.8724\n",
      "Epoch 00013: loss improved from 4.94811 to 4.87237, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 288ms/step - loss: 4.8724\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.7926\n",
      "Epoch 00014: loss improved from 4.87237 to 4.79259, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 278ms/step - loss: 4.7926\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.6946\n",
      "Epoch 00015: loss improved from 4.79259 to 4.69461, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 295ms/step - loss: 4.6946\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.6189\n",
      "Epoch 00016: loss improved from 4.69461 to 4.61886, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 294ms/step - loss: 4.6189\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.5347\n",
      "Epoch 00017: loss improved from 4.61886 to 4.53468, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 280ms/step - loss: 4.5347\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.4154\n",
      "Epoch 00018: loss improved from 4.53468 to 4.41544, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 276ms/step - loss: 4.4154\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.2741\n",
      "Epoch 00019: loss improved from 4.41544 to 4.27409, saving model to next_words.h5\n",
      "24/24 [==============================] - 7s 307ms/step - loss: 4.2741\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - ETA: 0s - loss: 4.1671\n",
      "Epoch 00020: loss improved from 4.27409 to 4.16711, saving model to next_words.h5\n",
      "24/24 [==============================] - 6s 265ms/step - loss: 4.1671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f124c05fdf0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=20, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "  \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "  \n",
    "  print(predicted_word)\n",
    "  return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: \n",
      "Execution stops\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "  \n",
    "  if text == \"\":\n",
    "      print(\"Execution stops\")\n",
    "      break\n",
    "  \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "        \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "          \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
